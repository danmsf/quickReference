{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If environment variables need to be set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare environment variables\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/spark/\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.7.0_67-cloudera/jre\"\n",
    "os.environ[\"YARN_CONF_DIR\"] = \"/etc/spark/conf.cloudera.spark_on_yarn/yarn-conf/\"\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages com.databricks:spark-csv_2.11:1.3.0,org.apache.hadoop:hadoop-aws:2.7.1 pyspark-shell\"\n",
    "# \"--name\" \"PySparkShell\" \"pyspark-shell\"\n",
    "\n",
    "\n",
    "# add to path environment variable\n",
    "import sys\n",
    "sys.path.insert(0, '/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/spark/python')\n",
    "sys.path.insert(0, '/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.23/lib/spark/python/lib/py4j-0.9-src.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise this is a standard setup with minimal memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import * # SqlContext, HiveContext \n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "\n",
    "conf = SparkConf = SparkConf().setMaster(\"yarn-client\") \\\n",
    ".setAppName(\"Dan-PySpark2\") \\\n",
    ".set(\"spark.driver.memory\", \"9g\") \\\n",
    ".set(\"spark.driver.cores\", \"3\") \\\n",
    ".set(\"spark.executor.cores\", \"3\") \\\n",
    ".set(\"spark.executor.memory\", \"9gb\") \\\n",
    ".set(\"spark.executor.instances\", \"2\")\n",
    "\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "hiveContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull Links\n",
    "\n",
    "* Configuration - (https://stackoverflow.com/questions/37871194/how-to-tune-spark-executor-number-cores-and-executor-memory/37871195#37871195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
